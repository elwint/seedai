import argparse
import json

debug=False

TYPE_SEQ2SEQ = "seq2seq"
TYPE_CAUSAL  = "causal"

def parse_args():
	default_parser = "goparser"
	default_model = "./ft-models/starcoder"
	default_type = TYPE_CAUSAL
	default_prompt_tuning = "no prompt tuning"
	default_legacy = False
	default_count = 10
	default_func = "Fuzz"
	default_corpus = "corpus"
	default_split = "\n\n###\n\n"
	default_debug = False

	parser = argparse.ArgumentParser(description="TODO.")

	parser.add_argument("--config", "-c", required=True,
					 help="generate config json file.")

	parser.add_argument("--parser", "-p", default=default_parser,
					 help=f"source code parser binary. Default is '{default_parser}'.")

	parser.add_argument("--model", "-m", default=default_model,
					 help=f"name of the LLM model to be used for seed generation. Default is '{default_model}'.")

	parser.add_argument("--type", "-t", default=default_type,
					 help=f"model type '{TYPE_CAUSAL}' or '{TYPE_SEQ2SEQ}'. Default is '{default_type}'.")

	parser.add_argument("--length", "-l", type=int, default=-1,
					 help="model max length. Default is tokenizer.model_max_length.")

	parser.add_argument("--prompt-tuning", "-pt", default=default_prompt_tuning,
					 help=f"enable prompt tuning config json file. Default is {default_prompt_tuning}.")

	parser.add_argument("--legacy", "-L", action="store_true", default=default_legacy,
					 help=f"enable legacy support (OpenAI). Default is {default_legacy}.")

	parser.add_argument("--count", "-C", type=int, default=default_count,
					 help=f"number of seeds to be generated by the LLM. Default is {default_count}.")

	parser.add_argument("--execs", "-E", type=int, default=-1,
					 help=f"number of simultaneous model executions. Default is equal to number of seeds.")

	parser.add_argument("--func", "-f", default=default_func,
					 help=f"name of the Fuzz function. Default is '{default_func}'.")

	parser.add_argument("--corpus",  "-d", default=default_corpus,
					 help=f"corpus directory. Default is '{default_corpus}'.")

	parser.add_argument("--split", "-s", default=default_split,
					 help="split string for causal model inference without prompt tuning. Default is {}.".
						format(json.dumps(default_split)))

	parser.add_argument("--debug", "--verbose", "-v", action="store_true", default=default_debug,
					 help=f"print debug output to debug.out. Default is {default_debug}.")

	args = parser.parse_args()
	if args.type not in [TYPE_CAUSAL, TYPE_SEQ2SEQ]:
		raise Exception("Invalid type")

	with open(args.config) as json_file:
		generate_args = json.load(json_file)

	if args.prompt_tuning != default_prompt_tuning:
		with open(args.prompt_tuning) as json_file:
			args.prompt_tuning = json.load(json_file)
	else:
		args.prompt_tuning = False

	if args.execs == -1:
		args.execs = args.count

	generate_args['execs'] = args.execs

	if args.debug:
		global debug
		debug = open('debug.out', 'w')

	return args, generate_args

def printd(v: str):
	if debug:
		print(v, file=debug, flush=True)
